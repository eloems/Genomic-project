{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea34235d",
   "metadata": {},
   "source": [
    "Group 4 data : https://owncloud.ulb.ac.be/index.php/s/OqS9s5qHtqbOFgE\n",
    "<br>\n",
    "Course material :  https://owncloud.ulb.ac.be/index.php/s/ffDcWZXxaeIzZVr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775681a8",
   "metadata": {},
   "source": [
    "# Informations from the project slides\n",
    "\n",
    "## Theoritical information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383a6e2",
   "metadata": {},
   "source": [
    "**Autoencoder (self supervised AI) :**\n",
    "<br>\n",
    "An autoencoder is a deep neural network\n",
    "trained to generate in its output the image it is\n",
    "presented as input\n",
    "The trick is that the information flows through\n",
    "an informational bottleneck\n",
    "Here the size of the bottleneck is 1% of the input\n",
    "image, thus the encoder must abstract out\n",
    "details and ignore noise\n",
    "The numerical vector defining bottleneck\n",
    "neurons activations elicited by an image is called\n",
    "its latent representation, it sits in a 512\n",
    "dimensions space, that we also refer to as the\n",
    "morphological space.\n",
    "<br>\n",
    "Our autoencoder does generate\n",
    "the input images, these are blurry\n",
    "versions of the original\n",
    "We are not interested in these\n",
    "reconstructed images, but in their\n",
    "latent representations\n",
    "These are points in a 512\n",
    "dimensions space, our poor brains\n",
    "can’t cope!\n",
    "So we collapse this space in 2D\n",
    "using the dimension reduction\n",
    "methods we routinely used in\n",
    "single cell transcriptomes analysis,\n",
    "e.g. t-SNE, UMAP, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83317ac5",
   "metadata": {},
   "source": [
    "**Contrastive learning (self supervised AI):**\n",
    "<br>\n",
    "SimCLR: a simple framework for **contrastive learning** of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.\n",
    "https://arxiv.org/abs/2002.05709"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f597c4",
   "metadata": {},
   "source": [
    "**Supervision with tasks loosely related to the final application:**\n",
    "<br>\n",
    "The AI underlying your project was trained to classify images\n",
    "from the Internet into 1000 categories of everyday life (e.g. cat, banana,\n",
    "train, table, ball, etc.).\n",
    "<br>\n",
    "This AI, has not seen any histological images during training, yet this is the\n",
    "best we got so far for our exploration of histology!\n",
    "<br>\n",
    "This is not completely unexpected after all: the fundamentals of\n",
    "pathologists’ vision were not learned in medical school, but in their cradle\n",
    "while they were babies randomly gazing at their environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca7dd69",
   "metadata": {},
   "source": [
    "**The Canguilhem study:**\n",
    "<br>\n",
    "*Normal vs pathological*\n",
    "* Criticized the definition of disease states as mere deviations from a norm\n",
    "* Very relevant to the thyroid cancer overtreatment problem\n",
    "\n",
    "\n",
    "**Thyroïd follicles:**\n",
    "* Main functional units of the thyroid are follicles containing colloid\n",
    "* Colloid is a viscous material composed predominantly of the thyroid hormone precursor protein\n",
    "<br>\n",
    "Thyroid nodules are found in examinations not related to thyroid disease (21% neck palpation, 67% ultrasonography)\n",
    "\n",
    "\n",
    "**Autopsy:**\n",
    "* Undiagnosed microcarcinomas in 22-36% of cases\n",
    "* Autoimmune thyroiditis (27% women, 7% men)\n",
    "\n",
    "\n",
    "**Proposition of the lab:**\n",
    "1. To survey the diversity of normal thyroid morphologies\n",
    "2. To achieve this we will develop a novel unbiased and quantitative approach to morphology\n",
    "\n",
    "\n",
    "**Aim of the lab:**\n",
    "* A: Survey the morphologies present in a large collection of asymptomatic thyroid histological slices\n",
    "* B: Survey the correlation of morphological variations with\n",
    "    *B1 : Genotypes\n",
    "    *B2 : Gene expression (our project)\n",
    "    *B3 : Clinical Data (our project)\n",
    "    \n",
    "We have computed morphological variations across 893 thyroid slices from GTEx\n",
    "<br>\n",
    "We are provided with pre-formated matrices\n",
    "\n",
    "\n",
    "**Overall goal of the project**\n",
    "<br>\n",
    "To compare the morphological description of tissues, thyroid glands, to their transcriptome\n",
    "<br>\n",
    "The morphological descriptions rests on unsupervised deep learning models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96761208",
   "metadata": {},
   "source": [
    "## Information about the samples and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40cd68e",
   "metadata": {},
   "source": [
    "**Samples (collected post-mortem):**\n",
    "<br>\n",
    "GTEx\n",
    "<br>\n",
    "*54 organs from 948 donors, most with :*\n",
    "* High-throughput genotypes\n",
    "* Genome-wide gene expression (RNA-seq)\n",
    "* Histology images (20X scans)\n",
    "* Donor level clinical annotations\n",
    "\n",
    "\n",
    "**clinical-data.tsv**\n",
    "<br>\n",
    "*Clinical data matrix includes :*\n",
    "* AGE\n",
    "* SEX \n",
    "* HEIGHT \n",
    "* WEIGHT \n",
    "* BMI \n",
    "* Cohort : organ donor or post-morthem\n",
    "* Ischemic time : time elapsed between the presumed donor death and tissue collection\n",
    "* Hardy Scale : 0 to 4 summarizing the circumstances of death\n",
    "* SUBJID : GTEx ID of the subject\n",
    "* **SMPLID : GTEx ID of the organ**\n",
    "* SMPTHNTS : sample’s pathology notes taken by GTEx pathologists who examined the histological slices\n",
    "* IMGURL : link to the interactively zoomable high resolution scan of the histological slice\n",
    "\n",
    "\n",
    "**RNA-read-counts.tsv**\n",
    "<br>\n",
    "**Gives the expression of genes across the samples.**\n",
    "<br>\n",
    "*Expression data :*\n",
    "* one row per transcripts (56200 in total)\n",
    "* First column : Transcript ENSEMBL IDs\n",
    "* Second column : Gene symbols\n",
    "* Other columns : each columns stands for a sample\n",
    "* Expression was recorded as raw, unormalized, read counts, i.e. a read count of 11 for gene G means that 11 reads align on G\n",
    "* Raw read counts are suitable inputs for differential gene expression software such as DESeq2 or edgeR\n",
    "* Column names of the expression matrix are indexed with **SMPLID**\n",
    "\n",
    "\n",
    "**morphological-counts.tsv**\n",
    "<br>\n",
    "Count matrix of the morphologies indentified by AI. \n",
    "<br>\n",
    "**Gives the expression of morphological clusters**\n",
    "<br>\n",
    "Suggest to use DeSeq2 or edgeR\n",
    "<br>\n",
    "<br>\n",
    "*Construction of the matrix :*\n",
    "1. Step 1: tile the high-resolution image\n",
    "    * 20X magnification\n",
    "    * Each tiles is 224x224 px 2 or 110x110 micron 2\n",
    "    * There are 5,000-10,000 tiles/image\n",
    "    * 7,000,000 tiles in the 893 thyroid images\n",
    "2. Step 2: compute tiles’ latent representations\n",
    "    * Neural Network : now a tile is a point in a 384 dimensional space\n",
    "3. Step 3: cluster tiles in latent space\n",
    "    * 64 morphological cluster are defined \n",
    "    * The clustering is done on the 7,000,000 tiles at once, so morphological clusters are representative of the 893 thyroids in GTEx\n",
    "    * Atlas of the morphological cluster = file morphological-atlas.pdf\n",
    "        * Order based on a (meta-) clustering of the morphological cluster counts.\n",
    "4. Step 4: cluster tiles in latent space\n",
    "    * Morphological summary = number of tiles computed in each cluster\n",
    "\n",
    "\n",
    "\n",
    "*Structure of the matrix :*\n",
    "1. Colums stands for morphological clusters (indexed from 0 to 63)\n",
    "2. **Line stand for samples (indexed by SMPLID)**\n",
    "3. Example\n",
    "    * matrix entry (GTEX-111CU-0226, Mophological-cluster-23) = 286\n",
    "    * Means thyroid GTEX-111CU-0226 has 286 tiles in Mophological-cluster-23\n",
    "    * Since tiles have a defined physical surface, 110x110 micron 2 (0.11x0.11 mm 2 ) it also mean that Mophological-cluster-23 spans 286x0.11 2 =3.5 mm 2 in histological image of thyroid GTEX-111CU-0226"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001dfd5",
   "metadata": {},
   "source": [
    "## Question 1 and hints - Exploration of clinical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241d361",
   "metadata": {},
   "source": [
    "1. Distribution \n",
    "2. Correlation\n",
    "3. **Are some technical variables possibly confounding demographic/health variables.**\n",
    "    \n",
    "*Hints :*\n",
    "<br>\n",
    "* PCA analytics\n",
    "* Variable's correlation matrix\n",
    "* Specific plot\n",
    "* DTHHRDY is more likely a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb297a",
   "metadata": {},
   "source": [
    "*Confounding technical variables:*\n",
    "<br>\n",
    "* Cohorte & Ischemic time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee64fc6",
   "metadata": {},
   "source": [
    "## Question 2 and hints - Clinical data vs morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09a31b",
   "metadata": {},
   "source": [
    "1. Compute systematically associations between clinical variables and morphological cluster counts. \n",
    "<br>\n",
    "The purpose is to compare the magnitude of the associations of the different variables with morphology.\n",
    "<br>\n",
    "2. Discuss the association with technical variables.\n",
    "3. For non-technical variables, redo the analysis with adjustment for the **confounding technical variables**, if any is reported in Q2.2.\n",
    "<br>\n",
    "Report and discuss significant associations.\n",
    "<br>\n",
    "<br>\n",
    "*Hints:*\n",
    "* It is a differential expression analysis where we study ‘differential morphological cluster expression’, use proven tool developed for transcript count data, like DESeq2, edgeR\n",
    "<br>\n",
    "<br>\n",
    "* Tools developed for transcript count data implements multivariate model formulas that will enable the treatment of **confounders in Q2.3**.\n",
    "    * Formula ‘~AGE+X+Y’ will compute the multivariate association of morphological clusters with variables AGE, X and Y.\n",
    "    * If you look at AGE from this model it will be adjusted for the variations of X and Y\n",
    "    * DESeq2 not only automatically handles normalization for total count and other intensity biases of count data, it also enables you to cancel out your variables of choice in the analysis.\n",
    "    * The function ‘as.formula’ (turns char string into a formula) could be useful to automate your analysis\n",
    "<br>\n",
    "<br>\n",
    "* Counts of different morphological clusters may be correlated with one another, as shown in the atlas.\n",
    "    * Patterns on a scale of 110x110 micron 2 may by part of the same larger-scale morphological entity.\n",
    "    * Ex : cluster 59 (colloid, liquid in the center of follicle) and 13 (edge of follicle) are both part of follicles so they tend to co-occur in the same samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54555476",
   "metadata": {},
   "source": [
    "## Question 3 and hints - Morphology vs gene expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a3c54",
   "metadata": {},
   "source": [
    "1. Report he number of significant down-regulated and up-regulated genes associated with each morphological cluster and the 10 most significant up-regulated genes. \n",
    "2. Question 3.1 with **Reactome gene sets**.\n",
    "3. Discuss the results, technically and biologically\n",
    "<br>\n",
    "<br>\n",
    "*Hints:*\n",
    "* Q3.1 : Use dedicated RNA-seq tools that take count data as input\n",
    "* Morphological cluster counts are directly related to image size, and image size depends on how the pathologist cut the organ as much as on its actual volume. It’s more relevant to look for the genes associated with morphological cluster proportions rather than raw counts.\n",
    "* I highly recommend you filter transcripts beforehand. You don’t need transcripts that are not/little expressed in the thyroid. It’s also a good idea to focus on transcripts showing high variability (use the median average deviation, a.k.a. mad() in R). Don’t hesitate to be drastic in your filtering (e.g. it’s OK to remove >50% of all transcripts)\n",
    "    * The filtering reduces the multiple testing problem, of course. But but it also reduce the computational burden. After appropriate filtering, I could run DESeq2 analyses for all 64 morphological clusters on 465 samples in ~1 hour and a peak RAM usage < 4GB\n",
    "* You may need to adjust for technical confounders\n",
    "* DESeq2, edgeR handle multiple testing. But you’ll have another layer of multiple testing since you will run these algorithms multiple times.\n",
    "<br>\n",
    "<br>\n",
    "* Surgeons sometime take muscle adjacent with the thyroid, that is **another technical confonder**.\n",
    "    * Cluster 14 = muscle cluster\n",
    "    * You may use it as an adjustment covariate in your analysis, rather than as a target variable.\n",
    "<br>\n",
    "<br>\n",
    "* Q3.2 : Running 64 times GSEA may not be computationally tractable. I suggest you use the **fgsea package**, which takes as input a gene ranking (i.e. provided by the differential analyses of Q3.1 output).\n",
    "    * It’s not using the the gold standard sample permutation of GSEA. But I tried it for you and it’s reasonable in this context where the transcriptional signal is quite massive.\n",
    "    * The REACTOME gene sets are provided in resource file c2.cp.reactome.v7.5.1.symbols.gmt. Fgsea provides a function to read *.gmt files. \n",
    "<br>\n",
    "<br>\n",
    "* Cluster 58 (tissue edges and various artifacts) and cluster 45 (dense lymphocytes aggregates) -> two remarkable clusters to guide Q3.3.\n",
    "    * Think of them as negative and positive controls.\n",
    "    * Can you estimate the prevalence of inflammation in your collection of asymptomatic thyroids?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d75fb8",
   "metadata": {},
   "source": [
    "## What the report should include : \n",
    "\n",
    "* A short introduction to the topic\n",
    "* A careful description of methods and results\n",
    "* A discussion of your results for each questions\n",
    "<br>\n",
    "<br>\n",
    "Send me the report by e-mail (Vincent.Detours@ulb.be), including ‘BINF-F401’ in the subject line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
